{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9e4369-ba84-4fd1-be75-763ac9a7d3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/MarwanRadi1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# needed for tokenizing the dataset\n",
    "nltk.download('punkt')\n",
    "# importing the regular expression\n",
    "import re\n",
    "# importing the numpy library to work with and manipulate the data\n",
    "import numpy as np\n",
    "# import the pandas library to read our dataset\n",
    "import pandas as pd\n",
    "# importing the sys library which can perform introspection about the system \n",
    "import sys\n",
    "# importing the os library to interact with our operating system\n",
    "import os\n",
    "# importing the load_model and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf11b54-6af3-4af3-bbdf-61522aa11321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model, Model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# importing the Bidirectional, Dense, TimeDistributed, LSTM, Embedding and Input layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bidirectional, Dense, TimeDistributed, LSTM, Embedding, Input\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _tf_keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/_tf_keras/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/_tf_keras/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/activations/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/src/backend/__init__.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Import backend functions.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf2xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_update_slice\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasVariable\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import load_model, Model\n",
    "# importing the Bidirectional, Dense, TimeDistributed, LSTM, Embedding and Input layers\n",
    "from keras.layers import Bidirectional, Dense, TimeDistributed, LSTM, Embedding, Input\n",
    "# setting the output length\n",
    "OUTPUT_LENGTH = 20\n",
    "# setting the output length\n",
    "INPUT_LENGTH = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd0670c-804b-4c1e-a4c4-e5deb91d3ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Chatbotfinalll.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1CQvwo7zz32X7WhULyK_LbPsZ3YMF8gUg\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "# importing the natural language tool kit\n",
    "import nltk\n",
    "# needed for tokenizing the dataset\n",
    "nltk.download('punkt')\n",
    "# importing the regular expression\n",
    "import re\n",
    "# importing the numpy library to work with and manipulate the data\n",
    "import numpy as np\n",
    "# import the pandas library to read our dataset\n",
    "import pandas as pd\n",
    "# importing the sys library which can perform introspection about the system \n",
    "import sys\n",
    "# importing the os library to interact with our operating system\n",
    "import os\n",
    "# importing the load_model and model\n",
    "from keras.models import load_model, Model\n",
    "# importing the Bidirectional, Dense, TimeDistributed, LSTM, Embedding and Input layers\n",
    "from keras.layers import Bidirectional, Dense, TimeDistributed, LSTM, Embedding, Input\n",
    "# setting the output length\n",
    "OUTPUT_LENGTH = 20\n",
    "# setting the output length\n",
    "INPUT_LENGTH = 20\n",
    "\n",
    "# it is for data loading\n",
    "# Reading two .txt files \n",
    "raw_movie_lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n') # The path to .txt file\n",
    "raw_movie_conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "# it is for mapping each line id with its corresponding text by creating a dictionary\n",
    "ansid = {}\n",
    "# reading lines from raw_movoe_lines dataset\n",
    "for line in raw_movie_lines:\n",
    "    # splitting the lines and assigning to _line\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    # if the len of line is equal to 5 then the below condition executes\n",
    "    if len(_line) == 5:\n",
    "        ansid[_line[0]] = _line[4]\n",
    "# Creating a list for all the conversations lines id's \n",
    "convs = []\n",
    "# reading the lines from raw_movie_conv_lines dataset\n",
    "for line in raw_movie_conv_lines[:-1]:\n",
    "    # splitting the lines and assigning to _line\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    # appending those splitted lines to conversations\n",
    "    convs.append(_line.split(','))\n",
    "# few set of id's and conversations \n",
    "for k in convs[300]:\n",
    "    # print the value of k\n",
    "    print (k, ansid[k])\n",
    "\n",
    "# We need to perform the sorting operations on sentences to questions and on answers \n",
    "# For questions\n",
    "ques = []\n",
    "# For Answers\n",
    "ans = []\n",
    "# for conversation in conversations\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        # appending the conversations ids to the questions\n",
    "        ques.append(ansid[conv[i]])\n",
    "        # appending the conversations ids to the answers\n",
    "        ans.append(ansid[conv[i+1]])\n",
    "# We need to Compare both the lengths of questions and answers\n",
    "# printing the length of the questions\n",
    "print(len(ques))\n",
    "# printing the length of the answers\n",
    "print(len(ans))\n",
    "\n",
    "# A method for cleaning the text \n",
    "def clean_text(text):\n",
    "    # it will be cleaning the text by removing all the unnedded characters and changing the words and characters format\n",
    "    # converting the text to the lower case letters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    # splitting the text and joining the text to the text\n",
    "    text = \" \".join(text.split())\n",
    "    # returning the text\n",
    "    return text\n",
    "# Cleaning the data which contains both questions and answers\n",
    "# For cleaning the questions\n",
    "clean_questions = []\n",
    "for question in ques:\n",
    "    # cleaning the questions and appending them to the clean_questions\n",
    "    clean_questions.append(clean_text(question))\n",
    "# For Cleaning the answers\n",
    "clean_answers = []    \n",
    "for answer in ans:\n",
    "    # cleaning the answers and appending them to the clean_answers\n",
    "    clean_answers.append(clean_text(answer))\n",
    "# After performing the clean opeartion on both questions and answers we need to find the length of the sentence\n",
    "# We are finding the length of sentences and we are not using the natural language tool kit during this process\n",
    "senlen = []\n",
    "for question in clean_questions:\n",
    "    # splitting and finding the length of the questions and then finally append it to the senlen\n",
    "    senlen.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    # splitting and finding the length of the answers and then finally append it to the senlen\n",
    "    senlen.append(len(answer.split()))\n",
    "# Need to create a dataframe which can be helpful in inspecting the values\n",
    "senlen = pd.DataFrame(senlen, columns=['counts'])\n",
    "print(np.percentile(senlen, 80))\n",
    "print(np.percentile(senlen, 85))\n",
    "print(np.percentile(senlen, 90))\n",
    "print(np.percentile(senlen, 95))\n",
    "\n",
    "# We need to remove both question and answers that are shorter than 1 word and larger than 20 words\n",
    "# so minimum length is 2\n",
    "minimum_length = 2\n",
    "# so maximum length is 20\n",
    "maximum_length = 20\n",
    "# Then filter the question and answer that are too short (or) long\n",
    "short_ques = []\n",
    "short_ans = []\n",
    "# for both index i and question in clean_questions\n",
    "for i, question in enumerate(clean_questions):\n",
    "    # checking the condition for length of the question is greater or equal to minimum_length and less than or equal to maximum_length \n",
    "    if len(question.split()) >= minimum_length and len(question.split()) <= maximum_length:\n",
    "        # if the condition is true\n",
    "        # then append the question to short questions\n",
    "        short_ques.append(question)\n",
    "        # if the condition is true\n",
    "        # then append the answers to the short answers\n",
    "        short_ans.append(clean_answers[i])\n",
    "short_qu = []\n",
    "short_an = []\n",
    "# for both index i and answer in short_answers\n",
    "for i, answer in enumerate(short_ans):\n",
    "    # checking the conditon for the length of the answer is greater or equal to minimum_length and less than or equal to maximum_length\n",
    "    if len(answer.split()) >= minimum_length and len(answer.split()) <= maximum_length:\n",
    "        # if the condition is true\n",
    "        # then append the answer to short answers\n",
    "        short_an.append(answer)\n",
    "        # if the condition is true\n",
    "        # then append the question to short questions\n",
    "        short_qu.append(short_ques[i])\n",
    "# printing the length of the short question               \n",
    "print(len(short_qu))\n",
    "# printing the length of the short answer\n",
    "print(len(short_an))\n",
    "\n",
    "# np.random.randint is used for returning the random integers \n",
    "r = np.random.randint(1,len(short_qu))\n",
    "for i in range(r, r+3):\n",
    "    # printing the short questions\n",
    "    print(short_qu[i])\n",
    "    # printing the short answers\n",
    "    print(short_an[i])\n",
    "    print()\n",
    "\n",
    "# Preprocessing for word based model \n",
    "#choosing the number of samples for training\n",
    "num_samples = 30000 \n",
    "# Number of samples for short questions \n",
    "short_qu = short_qu[:num_samples]\n",
    "# Number of samples to short answers\n",
    "short_an = short_an[:num_samples]\n",
    "# Applying the tokenizing on questions\n",
    "questions_tokenize = [nltk.word_tokenize(sent) for sent in short_qu]\n",
    "# Applying the tokenizing on answers\n",
    "answers_tokenize = [nltk.word_tokenize(sent) for sent in short_an]\n",
    "\n",
    "# splitting the training and validation\n",
    "# assigning the length of the tokenized questions to the value\n",
    "size_of_the_data = len(questions_tokenize)\n",
    "# We are using 80 percent of the data for the training and assigning it to the input_train_data\n",
    "input_train_data  = questions_tokenize[:round(size_of_the_data*(80/100))]\n",
    "# We are reverseing the input sequence for a good performance\n",
    "input_train_data  = [tr_input[::-1] for tr_input in input_train_data]\n",
    "# assigning value to the output trained data \n",
    "output_train_data = answers_tokenize[:round(size_of_the_data*(80/100))]\n",
    "# remaining 20% of the data is used for validations\n",
    "# assigning the value to the input_validating_data\n",
    "input_validating_data = questions_tokenize[round(size_of_the_data*(80/100)):]\n",
    "# We are reverseing the input sequence for a good performance\n",
    "input_validating_data  = [val_input[::-1] for val_input in input_validating_data]\n",
    "# assigning the value to the output_validating_data\n",
    "output_validating_data = answers_tokenize[round(size_of_the_data*(80/100)):]\n",
    "# printing the size of the training data\n",
    "print('The size of the training data', len(input_train_data))\n",
    "# printing the size of the validation data\n",
    "print('The size of the validation data', len(input_validating_data))\n",
    "\n",
    "# For vocabulary frequency we are creating a dictionary\n",
    "vocabulary = {}\n",
    "# for questions in questions_tokenize\n",
    "for question in questions_tokenize:\n",
    "    # for word in the question\n",
    "    for word in question:\n",
    "        # checking the condition if the word is not present in the vocabulary then assign 1\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 1\n",
    "        # if the word is present in the vocabulary then else part should get assigned\n",
    "        else:\n",
    "            vocabulary[word] += 1\n",
    "# for answer in answers_tokenize\n",
    "for answer in answers_tokenize:\n",
    "    # for word in answer\n",
    "    for word in answer:\n",
    "         # checking the condition if the word is not present in the vocabulary then assign 1\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 1\n",
    "        # if the word is present in the vocabulary then else part should get assigned\n",
    "        else:\n",
    "            vocabulary[word] += 1\n",
    "# We have to remove the rare words from the vocabulary and replace lesser than 5% of words with <UNK>\n",
    "# assign thresholdvalue to 15\n",
    "thresholdvalue = 15\n",
    "# assign count to 0\n",
    "count = 0\n",
    "for k,v in vocabulary.items():\n",
    "    # checking the condition if v is greater or equal to the thresholdvalue, then count gets increment\n",
    "    if v >= thresholdvalue:\n",
    "        count += 1\n",
    "# printing the total size of the vocabulary       \n",
    "print(\"Size of total vocabulary:\", len(vocabulary))\n",
    "# printing thr size of vocabulary in use\n",
    "print(\"Size of vocabulary we will use:\", count)\n",
    "\n",
    "# assign word_start to 1\n",
    "WORD_START = 1\n",
    "# assign word_padding to 0\n",
    "WORD_PADDING = 0\n",
    "# assign numberofword to 2\n",
    "numberoftheword  = 2\n",
    "# for encoded_data\n",
    "encoded_data = {}\n",
    "# for decoded_data\n",
    "decoded_data = {1: 'START'}\n",
    "# for word and count in vocabulary.items()\n",
    "for word, count in vocabulary.items():\n",
    "    # checking the condition if count is greater or equal to the thresholdvalue\n",
    "    if count >= thresholdvalue: \n",
    "        encoded_data[word] = numberoftheword \n",
    "        decoded_data[numberoftheword ] = word\n",
    "        numberoftheword += 1\n",
    "# printing the total number of vocabulary used\n",
    "print(\"Total number of vocabulary used:\", numberoftheword)\n",
    "\n",
    "# Dictionares are created which helps in providing the different integer for each and every word\n",
    "# including the unknown tokens for words and not for dictionary\n",
    "decoded_data[len(encoded_data)+2] = '<UNK>'\n",
    "encoded_data['<UNK>'] = len(encoded_data)+2\n",
    "# assigning the values to the dictionary_size\n",
    "dictionary_size = numberoftheword+1\n",
    "dictionary_size\n",
    "\n",
    "# Vectorizing the dataset\n",
    "# method for data transformation with inputs encoded_data and sizeofthevector\n",
    "# size of the vector is 20\n",
    "def datatransformation(encoded_data, data, sizeofthevector=20):\n",
    "    datatransformed = np.zeros(shape=(len(data), sizeofthevector))\n",
    "    # for i in the range of the data length\n",
    "    for i in range(len(data)):\n",
    "        # for j in the range of minimum length of the data\n",
    "        for j in range(min(len(data[i]), sizeofthevector)):\n",
    "            try:\n",
    "                datatransformed[i][j] = encoded_data[data[i][j]]\n",
    "            except:\n",
    "                datatransformed[i][j] = encoded_data['<UNK>']\n",
    "    # returning the datatransformed\n",
    "    return datatransformed\n",
    "\n",
    "# encoding the data training set\n",
    "# assigning the input trained data\n",
    "inputtrainingencoded = datatransformation(encoded_data, input_train_data, sizeofthevector=INPUT_LENGTH)\n",
    "# assigning the output trained data\n",
    "outputtrainingencoded = datatransformation(encoded_data, output_train_data, sizeofthevector=OUTPUT_LENGTH)\n",
    "# printing the input trained data\n",
    "print('The input training encoded value is:', inputtrainingencoded.shape)\n",
    "# printing the output trained data\n",
    "print('The output training encoded value is', outputtrainingencoded.shape)\n",
    "\n",
    "#encoding the data validation set\n",
    "# assigning the input validation data\n",
    "inputvalidationencoded = datatransformation(encoded_data, input_validating_data, sizeofthevector=INPUT_LENGTH)\n",
    "# assigning the output validation data\n",
    "outputvalidationencoded = datatransformation(encoded_data, output_validating_data, sizeofthevector=OUTPUT_LENGTH)\n",
    "# printing the input validation data\n",
    "print('The input validation encoded value is:', inputvalidationencoded.shape)\n",
    "# printing the output validation data\n",
    "print('The output validation encoded value is :', outputvalidationencoded.shape)\n",
    "\n",
    "# installing the tensorflow version of 1.14.0\n",
    "!pip install tensorflow==1.14.0\n",
    "# importing the tensorflow library\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Building the model\n",
    "# sequence-to-sequence operation in keras\n",
    "# assigning the input_length to 20\n",
    "INPUT_LENGTH = 20\n",
    "# assigning the output_length to 20\n",
    "OUTPUT_LENGTH = 20\n",
    "# assigning the value to encodedinput\n",
    "encodedinput = Input(shape=(INPUT_LENGTH,))\n",
    "# assigning the value to decodedinput\n",
    "decodedinput = Input(shape=(OUTPUT_LENGTH,))\n",
    "\n",
    "# import SimpleRNN from keras.layers\n",
    "from keras.layers import SimpleRNN\n",
    "# assigning the Embedding layer and its parameters to the encoding\n",
    "encoding = Embedding(dictionary_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encodedinput)\n",
    "# assigning the LSTM layer and its parameters to the encoding\n",
    "encoding = LSTM(512, return_sequences=True, unroll=True)(encoding)\n",
    "# assigning the value to the lastencoding\n",
    "lastencoding = encoding[:,-1,:]\n",
    "# printing the encoding value\n",
    "print('encoding', encoding)\n",
    "# printing the lastencoding value\n",
    "print('lastencoding', lastencoding)\n",
    "# assigning the Embedding layer and its parameters to the decoding\n",
    "decoding = Embedding(dictionary_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decodedinput)\n",
    "# assigning the LSTM layer and its parameters to the decoding\n",
    "decoding = LSTM(512, return_sequences=True, unroll=True)(decoding, initial_state=[lastencoding, lastencoding])\n",
    "# printing the decoding value\n",
    "print('decoding', decoding)\n",
    "\n",
    "# import dot, concatenate and activation from keras.layers\n",
    "from keras.layers import dot, concatenate, Activation\n",
    "# assigning the value to the layerss\n",
    "layerss = dot([decoding, encoding], axes=[2, 2])\n",
    "layerss = Activation('softmax', name='layerss')(layerss)\n",
    "# printing the layerss value\n",
    "print('layerss', layerss)\n",
    "layersss = dot([layerss, encoding], axes=[2,1])\n",
    "# printing the layerss value\n",
    "print('layerss', layerss)\n",
    "concatenate_context = concatenate([layerss, decoding])\n",
    "# printing the convatenate_context value\n",
    "print('concatenate_context', concatenate_context)\n",
    "# assigning the Dense layer and its parameters to the olayer with tanh activation function\n",
    "olayer = TimeDistributed(Dense(512, activation=\"tanh\"))(concatenate_context)\n",
    "# assigning the Dense layer and its parameters to the olayer with softmax activation function\n",
    "olayer = TimeDistributed(Dense(dictionary_size, activation=\"softmax\"))(olayer)\n",
    "# printing the olayer value\n",
    "print('olayer', olayer)\n",
    "\n",
    "# assigning the Model and its parameters to the mtrain model variable\n",
    "mtrain = Model(inputs=[encodedinput, decodedinput], outputs=[olayer])\n",
    "# compile the mtrain model with adam optimizer and binary_crossentropy loss\n",
    "mtrain.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summary of the mtrain model\n",
    "mtrain.summary()\n",
    "\n",
    "# assigning inputtrainingencoded to inputencodingtraining\n",
    "inputencodingtraining = inputtrainingencoded\n",
    "inputdecodingtraining = np.zeros_like(outputtrainingencoded)\n",
    "inputdecodingtraining[:, 1:] = outputtrainingencoded[:,:-1]\n",
    "# WORD_START to inputdecodingtraining\n",
    "inputdecodingtraining[:, 0] = WORD_START\n",
    "outputdecodingtraining = np.eye(dictionary_size)[outputtrainingencoded.astype('int')]\n",
    "# assigning inputvalidationencoded to inputencodingvalidation\n",
    "inputencodingvalidation = inputvalidationencoded\n",
    "inputdecodingvalidation = np.zeros_like(outputvalidationencoded)\n",
    "inputdecodingvalidation[:, 1:] = outputvalidationencoded[:,:-1]\n",
    "# assigning WORD_START to inputdecodingvalidation\n",
    "inputdecodingvalidation[:, 0] = WORD_START\n",
    "outputdecodingvalidation = np.eye(dictionary_size)[outputvalidationencoded.astype('int')]\n",
    "\n",
    "# assigning the batch_size=128, epochs=5 to mtrain model\n",
    "#history=mtrain.fit(x=[inputencodingtraining, inputdecodingtraining], y=[outputdecodingtraining],validation_data=([inputencodingvalidation, inputdecodingvalidation], [outputdecodingvalidation]),batch_size=128, epochs=2)\n",
    "history=mtrain.fit(x=[inputencodingtraining, inputdecodingtraining], y=[outputdecodingtraining],batch_size=64, epochs=5)\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "#history_df[['loss', 'val_loss']].plot()\n",
    "#history_df[['acc', 'val_acc']].plot()\n",
    "history_df[['loss']].plot() #Like so.\n",
    "history_df[['acc']].plot()\n",
    "\n",
    "# importing the load_model from keras.models\n",
    "from keras.models import load_model\n",
    "# It creates a HDF5 file\n",
    "mtrain.save('9_project2_TT.h5')\n",
    "# Deletes the model that currently exists\n",
    "del mtrain\n",
    "# loads the model\n",
    "mtrain = load_model('9_project2_TT.h5')\n",
    "mtrain.summary()\n",
    "\n",
    "#input_validating_data= input_validating_data.reshape(input_validating_data.shape[0], input_validating_data.shape[1], 1)\n",
    "#output_validating_data = output_validating_data.reshape(output_validating_data.shape[0], output_validating_data.shape[1], 1)\n",
    "#import numpy as np\n",
    "x_train_np = np.array(inputencodingvalidation)\n",
    "y_train_np = np.array(output_validating_data)\n",
    "x_test = np.expand_dims(x_train_np, axis=2)\n",
    "acc = mtrain.evaluate(x=[inputencodingvalidation, inputdecodingvalidation],y= [outputdecodingvalidation], verbose=0)\n",
    "print(\"Accuracy\"+str(acc))\n",
    "\n",
    "# method for predicting the raw input with the rawdata\n",
    "def predictingrawinput(rawdata):\n",
    "    # cleaning the rawdata\n",
    "    clean_textt = clean_text(rawdata)\n",
    "    # applying the tokenization on the cleaned text\n",
    "    inputtokenization = [nltk.word_tokenize(clean_textt)]\n",
    "    # The input sequence is reveersed\n",
    "    inputtokenization = [inputtokenization[0][::-1]] \n",
    "    # Data transfromation on encoded input\n",
    "    encodedinput = datatransformation(encoded_data, inputtokenization, 20)\n",
    "    decodedinput = np.zeros(shape=(len(encodedinput), OUTPUT_LENGTH))\n",
    "    # assigning the WORD_START to the decodedinput\n",
    "    decodedinput[:,0] = WORD_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        olayer = mtrain.predict([encodedinput, decodedinput]).argmax(axis=2)\n",
    "        decodedinput[:,i] = olayer[:,i]\n",
    "    # returning the olayer\n",
    "    return olayer\n",
    "# method for decoding the text with the decode_data and vectorization\n",
    "def decodingtext(decoded_data, vectorization):\n",
    "    text = ''\n",
    "    # for in in vectorization\n",
    "    for i in vectorization:\n",
    "        # checking the if condition for i equal to zero\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoded_data[i]\n",
    "    # returning the text\n",
    "    return text\n",
    "# for in in the range of 20    \n",
    "for i in range(20):\n",
    "    # assigning the value to the sequentialindex\n",
    "    sequentialindex = np.random.randint(1, len(short_qu))\n",
    "    # assigning the value to the olayer\n",
    "    olayer = predictingrawinput(short_qu[sequentialindex])\n",
    "    # printing the question :\n",
    "    print ('Questions Pls:', short_qu[sequentialindex])\n",
    "    # printing the answer :\n",
    "    print ('Your Answer:', decodingtext(decoded_data, olayer[0]))\n",
    "\n",
    "# assigning the input() to the rawdata\n",
    "rawdata = input()\n",
    "# assigning the rawdata to the olayer\n",
    "olayer = predictingrawinput(rawdata)\n",
    "# printing the decodingtext\n",
    "print (decodingtext(decoded_data, olayer[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c8485-fa0d-449b-9e0b-fe5e40c3872c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
